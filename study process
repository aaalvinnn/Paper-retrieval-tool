3.12
1、学习了网站爬取的基本知识，如通过网页检查或许所需对应信息的源代码标签，从而达到爬取的目的。爬取文章作者，序号，标题较为轻松，
但爬取日期时，因为文章日期在该页面没有显示，需要进入到论文的具体页面才能查看到，因此设计了一个子模块用于爬取文章的链接，再用
相同方法爬取某篇文章的日期（较为麻烦。后续思考如何优化）
2、爬取过程中也出现了一定的问题，如本次提交爬取页面时用的都是同一个用户头，疑似导致了被网站robot拦截的情况，从而爬取只能进行到
该页面的第14篇，下次提交将涉入随机虚拟用户头模块，以期解决
3.13
解决了第一项任务遗留的问题，目前基本完成了第一项任务所需要求（虽然代码并不是十分美观，并且索引的是2022的文章，最后更改下url即可）
1、准备第二项任务，在网上搜寻应该使用哪一种数据库。最初选择了简单轻量并封装在python的Sqlite,但后来发现sqlite是无服务器端的，运行于本地文件夹中的（虽然
也可以经过一定的处理手段间接连接服务端，但过于麻烦）于是选择了最常见的MySQL作为数据库。看到还要学习PHP语言和SQL语言，但找到了python的驱动（在安装mysql
时也可以安装python驱动包）
2、初步了解了一下flask框架，了解认识了为什么需要设计一个框架参与web应用开发（似乎发现了和数据库的联系？）。虽然在进行第二项任务时有点迷茫，目前大概找到
了方向
3.14
1、今日学习效率较高。学习了MySQL数据库如何使用，以及python-mysql-connection驱动数据库模块，学习了mysql数据库的可视化工具navicat。
2、代码编写方面，将第一项任务的保存论文信息至本地更改为了保存在本地服务器的mysql数据库中，编写了几个常用的增删改查数据库函数在另一个
文件（ms.py）中，本来想在主体文件中调用同级目录下的ms.py文件中自己写的函数，但发现这样运行虽然没有报错，却出现了无法将数据上传到mysql
中的情况，于是之后又将ms.py中的函数搬到了主体文件test1.py中。优化了一下爬取论文信息的字符串格式，使呈现在数据库中的信息更为美观。
3、学习了基础的http知识，了解了http语言的大体框架，并自己写了一个简单的可视化页面。
3.15
1、今日学习了html web页面编写（虽然感觉是前端的内容，但为了完成任务，只能照着网上的学一点基础的做个粗糙的界面吧。。）
   虽然也成功完成了在本地服务器（127.0.0.5000）写了一个网页，并且可以进行标题模糊查找、数据库序号精确查找、arXiv号精确查找，但html
   页面设计不太合理，仍存在知识欠缺，比如模糊查找从数据库中导出了部分论文信息后，暂时还没搞清楚怎么换行，导致全部堆积在一起，不甚凌乱。
   并且虽然将论文下载地址也一并附在了论文信息的后面，但不知如何通过html转换为hrep标签的超链接，接口还没有做的很好，所以前端就是让人头疼啊
   算是基本完成了第二项任务
