3.12
1、学习了网站爬取的基本知识，如通过网页检查或许所需对应信息的源代码标签，从而达到爬取的目的。爬取文章作者，序号，标题较为轻松，
但爬取日期时，因为文章日期在该页面没有显示，需要进入到论文的具体页面才能查看到，因此设计了一个子模块用于爬取文章的链接，再用
相同方法爬取某篇文章的日期（较为麻烦。后续思考如何优化）
2、爬取过程中也出现了一定的问题，如本次提交爬取页面时用的都是同一个用户头，疑似导致了被网站robot拦截的情况，从而爬取只能进行到
该页面的第14篇，下次提交将涉入随机虚拟用户头模块，以期解决
3.13
解决了第一项任务遗留的问题，目前基本完成了第一项任务所需要求（虽然代码并不是十分美观，并且索引的是2022的文章，最后更改下url即可）
1、准备第二项任务，在网上搜寻应该使用哪一种数据库。最初选择了简单轻量并封装在python的Sqlite,但后来发现sqlite是无服务器端的，运行于本地文件夹中的（虽然
也可以经过一定的处理手段间接连接服务端，但过于麻烦）于是选择了最常见的MySQL作为数据库。看到还要学习PHP语言和SQL语言，但找到了python的驱动（在安装mysql
时也可以安装python驱动包）
2、初步了解了一下flask框架，了解认识了为什么需要设计一个框架参与web应用开发（似乎发现了和数据库的联系？）。虽然在进行第二项任务时有点迷茫，目前大概找到
了方向
3.14
1、今日学习效率较高。学习了MySQL数据库如何使用，以及python-mysql-connection驱动数据库模块，学习了mysql数据库的可视化工具navicat。
2、代码编写方面，将第一项任务的保存论文信息至本地更改为了保存在本地服务器的mysql数据库中，编写了几个常用的增删改查数据库函数在另一个
文件（ms.py）中，本来想在主体文件中调用同级目录下的ms.py文件中自己写的函数，但发现这样运行虽然没有报错，却出现了无法将数据上传到mysql
中的情况，于是之后又将ms.py中的函数搬到了主体文件test1.py中。优化了一下爬取论文信息的字符串格式，使呈现在数据库中的信息更为美观。
3、学习了基础的http知识，了解了http语言的大体框架，并自己写了一个简单的可视化页面。
