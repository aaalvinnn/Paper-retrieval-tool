3.12
1、学习了网站爬取的基本知识，如通过网页检查或许所需对应信息的源代码标签，从而达到爬取的目的。爬取文章作者，序号，标题较为轻松，
但爬取日期时，因为文章日期在该页面没有显示，需要进入到论文的具体页面才能查看到，因此设计了一个子模块用于爬取文章的链接，再用
相同方法爬取某篇文章的日期（较为麻烦。后续思考如何优化）
2、爬取过程中也出现了一定的问题，如本次提交爬取页面时用的都是同一个用户头，疑似导致了被网站robot拦截的情况，从而爬取只能进行到
该页面的第14篇，下次提交将涉入随机虚拟用户头模块，以期解决
3.13
解决了第一项任务遗留的问题，目前基本完成了第一项任务所需要求（虽然代码并不是十分美观，并且索引的是2022的文章，最后更改下url即可）
1、准备第二项任务，在网上搜寻应该使用哪一种数据库。最初选择了简单轻量并封装在python的Sqlite,但后来发现sqlite是无服务器端的，运行于本地文件夹中的（虽然
也可以经过一定的处理手段间接连接服务端，但过于麻烦）于是选择了最常见的MySQL作为数据库。看到还要学习PHP语言和SQL语言，但找到了python的驱动（在安装mysql
时也可以安装python驱动包）
2、初步了解了一下flask框架，了解认识了为什么需要设计一个框架参与web应用开发（似乎发现了和数据库的联系？）。虽然在进行第二项任务时有点迷茫，目前大概找到
了方向
3.14
1、今日学习效率较高。学习了MySQL数据库如何使用，以及python-mysql-connection驱动数据库模块，学习了mysql数据库的可视化工具navicat。
2、代码编写方面，将第一项任务的保存论文信息至本地更改为了保存在本地服务器的mysql数据库中，编写了几个常用的增删改查数据库函数在另一个
文件（ms.py）中，本来想在主体文件中调用同级目录下的ms.py文件中自己写的函数，但发现这样运行虽然没有报错，却出现了无法将数据上传到mysql
中的情况，于是之后又将ms.py中的函数搬到了主体文件test1.py中。优化了一下爬取论文信息的字符串格式，使呈现在数据库中的信息更为美观。
3、学习了基础的http知识，了解了http语言的大体框架，并自己写了一个简单的可视化页面。
3.15
1、今日学习了html web页面编写（虽然感觉是前端的内容，但为了完成任务，只能照着网上的学一点基础的做个粗糙的界面吧。。）
   虽然也成功完成了在本地服务器（127.0.0.5000）写了一个网页，并且可以进行标题模糊查找、数据库序号精确查找、arXiv号精确查找，但html
   页面设计不太合理，仍存在知识欠缺，比如模糊查找从数据库中导出了部分论文信息后，暂时还没搞清楚怎么换行，导致全部堆积在一起，不甚凌乱。
   并且虽然将论文下载地址也一并附在了论文信息的后面，但不知如何通过html转换为hrep标签的超链接，接口还没有做的很好，所以前端就是让人头疼啊
   算是基本完成了第二项任务
3.16
今日满课，只有晚上的时间赶了点进度。中午在阿里云租了一个服务器，准备将web项目搬到云端，但怎么配置都在报错，实在有点破坏心态。
于是晚上决定先去做一个更新数据库的脚本。观察到arvix网站每天都会更新250篇左右的论文，于是依载datetime和threading库写了一个计时器，在
每天十点从arvix爬取当日的论文到数据库内（尚未完成）。
出现的问题：1、2020年的论文有15000多篇，一篇论文的pdf文件大概2mb，全下载下来可能需要30多G，可能出现服务器存储空间不足的情况。
2、在爬取arvix论文信息时，两次分别爬取了1535篇和714篇时，出现了arvix反爬脚本robot阻止接入的情况，短时间内被ban了ip地址。百度相关内容时
得到了以下两种方案：1、设置time.sleep,延迟访问网站的频率2、代理ip，用随机多个代理ip的方式访问网站
虽然仔细想了想，单纯爬取页面内容，是不会给arvix造成网站访问压力的。造成压力的原因在于，获取每篇文章发表日期时，需要进入每篇文章的页面，
所以才会短时间访问频率过高3、仍没有找到有效的web页面跑python程序的方法（或许是对flask框架理解的还不到位）
3.17
用一天的时间搭建了一个服务器，依托tomcat虚拟服务器，阿里云centOs8系统，创建了一个下载保存的pdf的目录（花费了大量时间）。前期一直在windows
系统的服务器搞，但后来发现服务器主流还是得看linux，并且还是与保存数据相关的。本来想顺便把web项目也搭载上去，但时间不够，只能草草了事。而且下载
地址是http协议，没有ssl，不太安全。同时整理了一下提交的文档，但失手删掉了第一天爬虫的源文件（永久删除哭辽），于是又花了点时间重码，严重耗费时间和精力
，但也让代码好看了点
